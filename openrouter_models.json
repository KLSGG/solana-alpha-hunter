{"data": [{"id": "openrouter/pony-alpha","canonical_slug": "openrouter/pony-alpha","hugging_face_id": "","name": "Pony Alpha","created": 1770393855,"description": "Pony is a cutting-edge foundation model with strong performance in coding, agentic workflows, reasoning, and roleplay, making it well suited for hands-on coding and real-world use.\\n\\n**Note:** All prompts and completions for this model are logged by the provider and may be used to improve the model.","context_length": 200000,"architecture": {"modality": "text->text","input_modalities": ["text"],"output_modalities": ["text"],"tokenizer": "Other","instruct_type": null},"pricing": {"prompt": "0","completion": "0","request": "0","image": "0","web_search": "0","internal_reasoning": "0"},"top_provider": {"context_length": 200000,"max_completion_tokens": 131000,"is_moderated": false},"per_request_limits": null,"supported_parameters": ["include_reasoning","max_tokens","reasoning","response_format","structured_outputs","temperature","tools","top_p"],"default_parameters": {"temperature": 1,"top_p": 0.95,"frequency_penalty": null},"expiration_date": null},{"id": "anthropic/claude-opus-4.6","canonical_slug": "anthropic/claude-4.6-opus-20260205","hugging_face_id": "","name": "Anthropic: Claude Opus 4.6","created": 1770219050,"description": "Opus 4.6 is Anthropic’s strongest model for coding and long-running professional tasks. It is built for agents that operate across entire workflows rather than single prompts, making it especially effective for large codebases, complex refactors, and multi-step debugging that unfolds over time. The model shows deeper contextual understanding, stronger problem decomposition, and greater reliability on hard engineering tasks than prior generations.\\n\\nBeyond coding, Opus 4.6 excels at sustained knowledge work. It produces near-production-ready documents, plans, and analyses in a single pass, and maintains coherence across very long outputs and extended sessions. This makes it a strong default for tasks that require persistence, judgment, and follow-through, such as technical design, migration planning, and end-to-end project execution.\\n\\nFor users upgrading from earlier Opus versions, see our [official migration guide here](https://openrouter.ai/docs/guides/guides/model-migrations/claude-4-6-opus)\\n\",\"context_length\": 1000000,\"architecture\": {"modality": "text+image->text","input_modalities": ["text","image"],"output_modalities": ["text"],"tokenizer": "Claude","instruct_type": null},"pricing": {"prompt": "0.000005","completion": "0.000025","web_search": "0.01","input_cache_read": "0.0000005","input_cache_write": "0.00000625"},"top_provider": {"context_length": 1000000,"max_completion_tokens": 128000,"is_moderated": true},"per_request_limits": null,"supported_parameters": ["include_reasoning","max_tokens","reasoning","response_format","stop","structured_outputs","temperature","tool_choice","tools","top_k","top_p","verbosity"],"default_parameters": {"temperature": null,"top_p": null,"frequency_penalty": null},"expiration_date": null},{"id": "qwen/qwen3-coder-next","canonical_slug": "qwen/qwen3-coder-next-2025-02-03","hugging_face_id": "Qwen/Qwen3-Coder-Next","name": "Qwen: Qwen3 Coder Next","created": 1770164101,"description": "Qwen3-Coder-Next is an open-weight causal language model optimized for coding agents and local development workflows. It uses a sparse MoE design with 80B total parameters and only 3B activated per token, delivering performance comparable to models with 10 to 20x higher active compute, which makes it well suited for cost-sensitive, always-on agent deployment.\\n\\nThe model is trained with a strong agentic focus and performs reliably on long-horizon coding tasks, complex tool usage, and recovery from execution failures. With a native 256k context window, it integrates cleanly into real-world CLI and IDE environments and adapts well to common agent scaffolds used by modern coding tools. The model operates exclusively in non-thinking mode and does not emit <think> blocks, simplifying integration for production coding agents.","context_length": 262144,"architecture": {"modality": "text->text","input_modalities": ["text"],"output_modalities": ["text"],"tokenizer": "Qwen","instruct_type": null},"pricing": {"prompt": "0.00000007","completion": "0.0000003","input_cache_read": "0.000000035"},"top_provider": {"context_length": 262144,"max_completion_tokens": 65536,"is_moderated": false},"per_request_limits": null,"supported_parameters": ["frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_k","top_p"],"default_parameters": {"temperature": 1,"top_p": 0.95,"frequency_penalty": null},"expiration_date": null},{"id": "openrouter/free","canonical_slug": "openrouter/free","hugging_face_id": "","name": "Free Models Router","created": 1769917427,"description": "The simplest way to get free inference. openrouter/free is a router that selects free models at random from the models available on OpenRouter. The router smartly filters for models that support features needed for your request such as image understanding, tool calling, structured outputs and more. ","context_length": 200000,"architecture": {"modality": "text+image->text","input_modalities": ["text","image"],"output_modalities": ["text"],"tokenizer": "Router","instruct_type": null},"pricing": {"prompt": "0","completion": "0"},"top_provider": {"context_length": null,"max_completion_tokens": null,"is_moderated": false},"per_request_limits": null,"supported_parameters": ["frequency_penalty","include_reasoning","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_k","top_p"],"default_parameters": {"temperature": null,"top_p": null,"frequency_penalty": null},"expiration_date": null},{"id": "stepfun/step-3.5-flash:free","canonical_slug": "stepfun/step-3.5-flash","hugging_face_id": "stepfun-ai/Step-3.5-Flash","name": "StepFun: Step 3.5 Flash (free)","created": 1769728337,"description": "Step 3.5 Flash is StepFun's most capable open-source foundation model. Built on a sparse Mixture of Experts (MoE) architecture, it selectively activates only 11B of its 196B parameters per token. It is a reasoning model that is incredibly speed efficient even at long contexts.","context_length": 256000,"architecture": {"modality": "text->text","input_modalities": ["text"],"output_modalities": ["text"],"tokenizer": "Other","instruct_type": null},"pricing": {"prompt": "0","completion": "0"},"top_provider": {"context_length": 256000,"max_completion_tokens": 256000,"is_moderated": false},"per_request_limits": null,"supported_parameters": ["frequency_penalty","include_reasoning","max_tokens","reasoning","stop","temperature","tools","top_p"],"default_parameters": {"temperature": null,"top_p": null,"frequency_penalty": null},"expiration_date": null},{"id": "arcee-ai/trinity-large-preview:free","canonical_slug": "arcee-ai/trinity-large-preview","hugging_face_id": "arcee-ai/Trinity-Large-Preview","name": "Arcee AI: Trinity Large Preview (free)","created": 1769552670,"description": "Trinity-Large-Preview is a frontier-scale open-weight language model from Arcee, built as a 400B-parameter sparse Mixture-of-Experts with 13B active parameters per token using 4-of-256 expert routing. \\n\\nIt excels in creative writing, storytelling, role-play, chat scenarios, and real-time voice assistance, better than your average reasoning model usually can. But we’re also introducing some of our newer agentic performance. It was trained to navigate well in agent harnesses like OpenCode, Cline, and Kilo Code, and to handle complex toolchains and long, constraint-filled prompts. \\n\\nThe architecture natively supports very long context windows up to 512k tokens, with the Preview API currently served at 128k context using 8-bit quantization for practical deployment. Trinity-Large-Preview reflects Arcee’s efficiency-first design philosophy, offering a production-oriented frontier model with open weights and permissive licensing suitable for real-world applications and experimentation.","context_length": 131000,"architecture": {"modality": "text->text","input_modalities": ["text"],"output_modalities": ["text"],"tokenizer": "Other","instruct_type": null},"pricing": {"prompt": "0","completion": "0"},"top_provider": {"context_length": 131000,"max_completion_tokens": null,"is_moderated": false},"per_request_limits": null,"supported_parameters": ["max_tokens","response_format","structured_outputs","temperature","tools","top_k","top_p"],"default_parameters": {"temperature": 0.8,"top_p": 0.8,"frequency_penalty": null},"expiration_date": null},{"id": "moonshotai/kimi-k2.5","canonical_slug": "moonshotai/kimi-k2.5-0127","hugging_face_id": "moonshotai/Kimi-K2.5","name": "MoonshotAI: Kimi K2.5","created": 1769487076,"description": "Kimi K2.5 is Moonshot AI's native multimodal model, delivering state-of-the-art visual coding capability and a self-directed agent swarm paradigm. Built on Kimi K2 with continued pretraining over approximately 15T mixed visual and text tokens, it delivers strong performance in general reasoning, visual coding, and agentic tool-calling.","context_length": 262144,"architecture": {"modality": "text+image->text","input_modalities": ["text","image"],"output_modalities": ["text"],"tokenizer": "Other","instruct_type": null},"pricing": {"prompt": "0.0000003","completion": "0.0000015","input_cache_read": "0.00000005"},"top_provider": {"context_length": 262144,"max_completion_tokens": null,"is_moderated": false},"per_request_limits": null,"supported_parameters": ["frequency_penalty","include_reasoning","logit_bias","logprobs","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_k","top_logprobs","top_p"],"default_parameters": {"temperature": null,"top_p": null,"frequency_penalty": null},"expiration_date": null},{"id": "upstage/solar-pro-3:free","canonical_slug": "upstage/solar-pro-3","hugging_face_id": "","name": "Upstage: Solar Pro 3 (free)","created": 1769481200,"description": "Solar Pro 3 is Upstage's powerful Mixture-of-Experts (MoE) language model. With 102B total parameters and 12B active parameters per forward pass, it delivers exceptional performance while maintaining computational efficiency. Optimized for Korean with English and Japanese support.","context_length": 128000,"architecture": {"modality": "text->text","input_modalities": ["text"],"output_modalities": ["text"],"tokenizer": "Other","instruct_type": null},"pricing": {"prompt": "0","completion": "0"},"top_provider": {"context_length": 128000,"max_completion_tokens": null,"is_moderated": false},"per_request_limits": null,"supported_parameters": ["include_reasoning","max_tokens","reasoning","response_format","structured_outputs","temperature","tool_choice","tools"],"default_parameters": {"temperature": null,"top_p": null,"frequency_penalty": null},"expiration_date": null},{"id": "minimax/minimax-m2-her","canonical_slug": "minimax/minimax-m2-her-20260123","hugging_face_id": "","name": "MiniMax: MiniMax M2-her","created": 1769177239,"description": "MiniMax M2-her is a dialogue-first large language model built for immersive roleplay, character-driven chat, and expressive multi-turn conversations. Designed to stay consistent in tone and personality, it supports rich message roles (user_system, group, sample_message_user, sample_message_ai) and can learn from example dialogue to better match the style and pacing of your scenario, making it a strong choice for storytelling, companions, and conversational experiences where natural flow and vivid interaction matter most.","context_length": 65536,"architecture": {"modality": "text->text","input_modalities": ["text"],"output_modalities": ["text"],"tokenizer": "Other","instruct_type": null},"pricing": {"prompt": "0.0000003","completion": "0.0000012","input_cache_read": "0.00000003"},"top_provider": {"context_length": 65536,"max_completion_tokens": 2048,"is_moderated": false},"per_request_limits": null,"supported_parameters": ["max_tokens","temperature","top_p"],"default_parameters": {"temperature": 1,"top_p": 0.95,"frequency_penalty": null},"expiration_date": null},{"id": "writer/palmyra-x5","canonical_slug": "writer/palmyra-x5-20250428","hugging_face_id": "","name": "Writer: Palmyra X5","created": 1769003823,"description": "Palmyra X5 is Writer's most advanced model, purpose-built for building and scaling AI agents across the enterprise. It delivers industry-leading speed and efficiency on context windows up to 1 million tokens, powered by a novel transformer architecture and hybrid attention mechanisms. This enables faster inference and expanded memory for processing large volumes of enterprise data, critical for scaling AI agents.","context_length": 1040000,"architecture": {"modality": "text->text","input_modalities": ["text"],"output_modalities": ["text"],"tokenizer": "Other","instruct_type": null},"pricing": {"prompt": "0.0000006","completion": "0.000006"},"top_provider": {"context_length": 1040000,"max_completion_tokens": 8192,"is_moderated": true},"per_request_limits": null,"supported_parameters": ["max_tokens","stop","temperature","top_k","top_p"],"default_parameters": {"temperature": null,"top_p": null,"frequency_penalty": null},"expiration_date": null},{"id": "liquid/lfm-2.5-1.2b-thinking:free","canonical_slug": "liquid/lfm-2.5-1.2b-thinking-20260120","hugging_face_id": "LiquidAI/LFM2.5-1.2B-Thinking","name": "LiquidAI: LFM2.5-1.2B-Thinking (free)","created": 1768927527,"description": "LFM2.5-1.2B-Thinking is a lightweight reasoning-focused model optimized for agentic tasks, data extraction, and RAG—while still running comfortably on edge devices. It supports long context (up to 32K tokens) and is designed to provide higher-quality “thinking” responses in a small 1.2B model.","context_length": 32768,"architecture": {"modality": "text->text","input_modalities": ["text"],"output_modalities": ["text"],"tokenizer": "Other","instruct_type": null},"pricing": {"prompt": "0","completion": "0"},"top_provider": {"context_length": 32768,"max_completion_tokens": null,"is_moderated": false},"per_request_limits": null,"supported_parameters": ["frequency_penalty","include_reasoning","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","seed","stop","temperature","top_k","top_p"],"default_parameters": {"temperature": null,"top_p": null,"frequency_penalty": null},"expiration_date": null},{"id": "liquid/lfm-2.5-1.2b-instruct:free","canonical_slug": "liquid/lfm-2.5-1.2b-instruct-20260120","hugging_face_id": "LiquidAI/LFM2.5-1.2B-Instruct","name": "LiquidAI: LFM2.5-1.2B-Instruct (free)","created": 1768927521,"description": "LFM2.5-1.2B-Instruct is a compact, high-performance instruction-tuned model built for fast on-device AI. It delivers strong chat quality in a 1.2B parameter footprint, with efficient edge inference and broad runtime support.","context_length": 32768,"architecture": {"modality": "text->text","input_modalities": ["text"],"output_modalities": ["text"],"tokenizer": "Other","instruct_type": null},"pricing": {"prompt": "0","completion": "0"},"top_provider": {"context_length": 32768,"max_completion_tokens": null,"is_moderated": false},"per_request_limits": null,"supported_parameters": ["frequency_penalty","max_tokens","min_p","presence_penalty","repetition_penalty","seed","stop","temperature","top_k","top_p"],"default_parameters": {"temperature": null,"top_p": null,"frequency_penalty": null},"expiration_date": null},{"id": "openai/gpt-audio","canonical_slug": "openai/gpt-audio","hugging_face_id": "","name": "OpenAI: GPT Audio","created": 1768862569,"description": "The gpt-audio model is OpenAI's first generally available audio model. The new snapshot features an upgraded decoder for more natural sounding voices and maintains better voice consistency. Audio is priced at $32 per million input tokens and $64 per million input tokens.","context_length": 128000,"architecture": {"modality": "text+audio->text+audio","input_modalities": ["text","audio"],"output_modalities": ["text","audio"],"tokenizer": "GPT","instruct_type": null},"pricing": {"prompt": "0.0000025","completion": "0.00001","audio": "0.000032"},"top_provider": {"context_length": 128000,"max_completion_tokens": 16384,"is_moderated": true},"per_request_limits": null,"supported_parameters": ["frequency_penalty","logit_bias","logprobs","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","temperature","top_logprobs","top_p"],"default_parameters": {"temperature": null,"top_p": null,"frequency_penalty": null},"expiration_date": null},{"id": "openai/gpt-audio-mini","canonical_slug": "openai/gpt-audio-mini","hugging_face_id": "","name": "OpenAI: GPT Audio Mini","created": 1768859419,"description": "A cost-efficient version of GPT Audio. The new snapshot features an upgraded decoder for more natural sounding voices and maintains better voice consistency. Input is priced at $0.60 per million tokens and output is priced at $2.40 per million tokens.","context_length": 128000,"architecture": {"modality": "text+audio->text+audio","input_modalities": ["text","audio"],"output_modalities": ["text","audio"],"tokenizer": "GPT","instruct_type": null},"pricing": {"prompt": "0.0000006","completion": "0.0000024","audio": "0.0000006"},"top_provider": {"context_length": 128000,"max_completion_tokens": 16384,"is_moderated": true},"per_request_limits": null,"supported_parameters": ["frequency_penalty","logit_bias","logprobs","max_tokens","presence_penalty","response_format","seed","stop","structured_outputs","temperature","top_logprobs","top_p"],"default_parameters": {"temperature": null,"top_p": null,"frequency_penalty": null},"expiration_date": null},{"id": "z-ai/glm-4.7-flash","canonical_slug": "z-ai/glm-4.7-flash-20260119","hugging_face_id": "zai-org/GLM-4.7-Flash","name": "Z.AI: GLM 4.7 Flash","created": 1768833913,"description": "As a 30B-class SOTA model, GLM-4.7-Flash offers a new option that balances performance and efficiency. It is further optimized for agentic coding use cases, strengthening coding capabilities, long-horizon task planning, and tool collaboration, and has achieved leading performance among open-source models of the same size on several current public benchmark leaderboards.","context_length": 202752,"architecture": {"modality": "text->text","input_modalities": ["text"],"output_modalities": ["text"],"tokenizer": "Other","instruct_type": null},"pricing": {"prompt": "0.00000006","completion": "0.0000004","input_cache_read": "0.0000000100000002"},"top_provider": {"context_length": 202752,"max_completion_tokens": null,"is_moderated": false},"per_request_limits": null,"supported_parameters": ["frequency_penalty","include_reasoning","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_k","top_p"],"default_parameters": {"temperature": 1,"top_p": 0.95,"frequency_penalty": null},"expiration_date": null},{"id": "google/gemini-3-flash-preview","canonical_slug": "google/gemini-3-flash-preview-20251217","hugging_face_id": "","name": "Google: Gemini 3 Flash Preview","created": 1765987078,"description": "Gemini 3 Flash Preview is a high speed, high value thinking model designed for agentic workflows, multi turn chat, and coding assistance. It delivers near Pro level reasoning and tool use performance with substantially lower latency than larger Gemini variants, making it well suited for interactive development, long running agent loops, and collaborative coding tasks. Compared to Gemini 2.5 Flash, it provides broad quality improvements across reasoning, multimodal understanding, and reliability.\\n\\nThe model supports a 1M token context window and multimodal inputs including text, images, audio, video, and PDFs, with text output. It includes configurable reasoning via thinking levels (minimal, low, medium, high), structured output, tool use, and automatic context caching. Gemini 3 Flash Preview is optimized for users who want strong reasoning and agentic behavior without the cost or latency of full scale frontier models.","context_length": 1048576,"architecture": {"modality": "text+image+file+audio+video->text","input_modalities": ["text","image","file","audio","video"],"output_modalities": ["text"],"tokenizer": "Gemini","instruct_type": null},"pricing": {"prompt": "0.0000005","completion": "0.000003","image": "0.0000005","audio": "0.000001","internal_reasoning": "0.000003","input_cache_read": "0.00000005","input_cache_write": "0.00000008333333333333334"},"top_provider": {"context_length": 1048576,"max_completion_tokens": 65535,"is_moderated": false},"per_request_limits": null,"supported_parameters": ["include_reasoning","max_tokens","reasoning","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_p"],"default_parameters": {"temperature": null,"top_p": null,"frequency_penalty": null},"expiration_date": null},{"id": "mistralai/mistral-small-creative","canonical_slug": "mistralai/mistral-small-creative-20251216","hugging_face_id": null,"name": "Mistral: Mistral Small Creative","created": 1765908653,"description": "Mistral Small Creative is an experimental small model designed for creative writing, narrative generation, roleplay and character-driven dialogue, general-purpose instruction following, and conversational agents.","context_length": 32768,"architecture": {"modality": "text->text","input_modalities": ["text"],"output_modalities": ["text"],"tokenizer": "Mistral","instruct_type": null},"pricing": {"prompt": "0.0000001","completion": "0.0000003"},"top_provider": {"context_length": 32768,"max_completion_tokens": null,"is_moderated": false},"per_request_limits": null,"supported_parameters": ["tool_choice","tools"],"default_parameters": {"temperature": 0.3,"top_p": 0.95,"frequency_penalty": null},"expiration_date": null},{"id": "allenai/olmo-3.1-32b-think","canonical_slug": "allenai/olmo-3.1-32b-think-20251215","hugging_face_id": "allenai/Olmo-3.1-32B-Think","name": "AllenAI: Olmo 3.1 32B Think","created": 1765907719,"description": "Olmo 3.1 32B Think is a large-scale, 32-billion-parameter model designed for deep reasoning, complex multi-step logic, and advanced instruction following. Building on the Olmo 3 series, version 3.1 delivers refined reasoning behavior and stronger performance across demanding evaluations and nuanced conversational tasks. Developed by Ai2 under the Apache 2.0 license, Olmo 3.1 32B Think continues the Olmo initiative’s commitment to openness, providing full transparency across model weights, code, and training methodology.","context_length": 65536,"architecture": {"modality": "text->text","input_modalities": ["text"],"output_modalities": ["text"],"tokenizer": "Other","instruct_type": null},"pricing": {"prompt": "0.00000015","completion": "0.0000005"},"top_provider": {"context_length": 65536,"max_completion_tokens": 65536,"is_moderated": false},"per_request_limits": null,"supported_parameters": ["frequency_penalty","include_reasoning","logit_bias","max_tokens","min_p","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","top_k","top_p"],"default_parameters": {"temperature": 0.6,"top_p": 0.95,"frequency_penalty": null},"expiration_date": null},{"id": "xiaomi/mimo-v2-flash","canonical_slug": "xiaomi/mimo-v2-flash-20251210","hugging_face_id": "XiaomiMiMo/MiMo-V2-Flash","name": "Xiaomi: MiMo-V2-Flash","created": 1765731308,"description": "MiMo-V2-Flash is an open-source foundation language model developed by Xiaomi. It is a Mixture-of-Experts model with 309B total parameters and 15B active parameters, adopting hybrid attention architecture. MiMo-V2-Flash supports a hybrid-thinking toggle and a 256K context window, and excels at reasoning, coding, and agent scenarios. On SWE-bench Verified and SWE-bench Multilingual, MiMo-V2-Flash ranks as the top #1 open-source model globally, delivering performance comparable to Claude Sonnet 4.5 while costing only about 3.5% as much.\\n\\nUsers can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config).","context_length": 262144,"architecture": {"modality": "text->text","input_modalities": ["text"],"output_modalities": ["text"],"tokenizer": "Other","instruct_type": null},"pricing": {"prompt": "0.00000009","completion": "0.00000029","input_cache_read": "0.000000045"},"top_provider": {"context_length": 262144,"max_completion_tokens": null,"is_moderated": false},"per_request_limits": null,"supported_parameters": ["frequency_penalty","include_reasoning","max_tokens","presence_penalty","reasoning","repetition_penalty","response_format","seed","stop","structured_outputs","temperature","tool_choice","tools","top_k","top_p"],"default_parameters": {"temperature": null,"top_p": 0.95,"frequency_penalty": null},"expiration_date": null},{"id": "nvidia/nemotron-3-nano-30b-a3b:free","canonical_slug": "nvidia/nemotron-3-nano-30b-a3b","hugging_face_id": "nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16","name": "NVIDIA: Nemotron 3 Nano 30B A3B (free)","created": 1765731275,"description": "NVIDIA Nemotron 3 Nano 30B A3B is a small language MoE model with highest compute efficiency and accuracy for developers to build specialized agentic AI systems.\\n\\nThe model is fully open with open-weights, datasets and recipes so developers can easily\\ncustomize, optimize, and deploy the model on their infrastructure for maximum privacy and\\nsecurity.\\n\\nNote: For the free endpoint, all prompts and output are logged to improve the provider's model and its product and services. Please do not upload any personal, confidential, or otherwise sensitive information. This is a trial use only. Do not use for production or business-critical systems.","context_length": 256000,"architecture": {"modality": "text->text","input_modalities": ["text"],"output_modalities": ["text"],"tokenizer": "Other","instruct_type": null},"pricing": {"prompt": "0","completion": "0"},"top_provider": {"context_length": 256000,"max_completion_tokens": null,"is_moderated": false},"per_request_limits": null,"supported_parameters": ["include_reasoning","max_tokens","reasoning","seed","temperature","tool_choice","tools","top_p"],"default_parameters": {"temperature": null,"top_p": null,"frequency_penalty": null},"expiration_date": null}]}